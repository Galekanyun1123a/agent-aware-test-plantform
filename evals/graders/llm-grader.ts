/**
 * LLM Grader - LLM è¯„åˆ†å™¨
 *
 * ä½¿ç”¨ LLM åŸºäº Rubric è¿›è¡Œä»£ç è´¨é‡è¯„ä¼°
 * æ”¯æŒå¤šç§åç«¯ï¼š
 * - æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼šåŸºäºä»£ç ç‰¹å¾è¿›è¡Œå¯å‘å¼è¯„åˆ†
 * - OpenAI APIï¼šä½¿ç”¨ GPT-4 è¿›è¡Œè¯„ä¼°
 * - Anthropic APIï¼šä½¿ç”¨ Claude è¿›è¡Œè¯„ä¼°
 * 
 * é€šè¿‡ç¯å¢ƒå˜é‡ EVAL_LLM_BACKEND æ§åˆ¶ï¼š
 * - 'mock' (é»˜è®¤)ï¼šä½¿ç”¨æ¨¡æ‹Ÿè¯„åˆ†
 * - 'openai'ï¼šä½¿ç”¨ OpenAI GPT-4
 * - 'anthropic'ï¼šä½¿ç”¨ Anthropic Claude
 */

import fs from 'node:fs';
import path from 'node:path';
import type { LLMGraderConfig, GraderResult } from '../harness/types';
import { collectCodeContent } from '../harness/environment';

// LLM è¯„åˆ†ç»“æœç±»å‹
interface LLMGradingResult {
  dimensions: Record<string, number>;
  overall: number;
  reasoning: string;
}

// è·å– LLM åç«¯é…ç½®
const LLM_BACKEND = process.env.EVAL_LLM_BACKEND || 'mock';
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const ANTHROPIC_API_KEY = process.env.ANTHROPIC_API_KEY;

/**
 * è¯»å– Rubric æ–‡ä»¶
 */
function readRubric(rubricPath: string): string | null {
  const fullPath = path.join(process.cwd(), 'evals', 'rubrics', rubricPath);

  if (!fs.existsSync(fullPath)) {
    return null;
  }

  return fs.readFileSync(fullPath, 'utf-8');
}

/**
 * æ„å»ºè¯„ä¼° Prompt
 */
function buildEvalPrompt(
  rubric: string,
  dimensions: string[],
  codeContent: string
): string {
  // é™åˆ¶ä»£ç é•¿åº¦ï¼Œé¿å…è¶…å‡º token é™åˆ¶
  const maxCodeLength = 15000;
  const truncatedCode = codeContent.length > maxCodeLength 
    ? codeContent.slice(0, maxCodeLength) + '\n\n... (ä»£ç å·²æˆªæ–­)'
    : codeContent;

  return `ä½ æ˜¯ä¸€ä¸ªä»£ç è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¯„åˆ†æ ‡å‡†è¯„ä¼°ä»£ç ã€‚

## è¯„åˆ†æ ‡å‡†

${rubric}

## è¯„ä¼°ç»´åº¦

${dimensions.map((d, i) => `${i + 1}. ${d}`).join('\n')}

## å¾…è¯„ä¼°ä»£ç 

\`\`\`
${truncatedCode}
\`\`\`

## è¾“å‡ºæ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š
{
  "dimensions": {
    "${dimensions[0]}": 0.8${dimensions.length > 1 ? `,\n    "${dimensions[1]}": 0.9` : ''}
  },
  "overall": 0.85,
  "reasoning": "è¯„åˆ†ç†ç”±..."
}

æ³¨æ„ï¼š
1. æ¯ä¸ªç»´åº¦åˆ†æ•°åœ¨ 0-1 ä¹‹é—´
2. overall æ˜¯æ‰€æœ‰ç»´åº¦çš„åŠ æƒå¹³å‡
3. reasoning è¯·ç®€è¦è¯´æ˜è¯„åˆ†ç†ç”±ï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰`;
}

/**
 * ä½¿ç”¨ OpenAI API è°ƒç”¨ LLM
 */
async function callOpenAI(prompt: string): Promise<LLMGradingResult> {
  if (!OPENAI_API_KEY) {
    throw new Error('OPENAI_API_KEY æœªé…ç½®');
  }

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${OPENAI_API_KEY}`,
    },
    body: JSON.stringify({
      model: 'gpt-4-turbo-preview',
      messages: [{ role: 'user', content: prompt }],
      temperature: 0,
      response_format: { type: 'json_object' },
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`OpenAI API è°ƒç”¨å¤±è´¥: ${response.status} - ${error}`);
  }

  const data = await response.json();
  const content = data.choices?.[0]?.message?.content;
  
  if (!content) {
    throw new Error('OpenAI è¿”å›ç©ºå“åº”');
  }

  return parseJsonResponse(content);
}

/**
 * ä½¿ç”¨ Anthropic API è°ƒç”¨ LLM
 */
async function callAnthropic(prompt: string): Promise<LLMGradingResult> {
  if (!ANTHROPIC_API_KEY) {
    throw new Error('ANTHROPIC_API_KEY æœªé…ç½®');
  }

  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': ANTHROPIC_API_KEY,
      'anthropic-version': '2023-06-01',
    },
    body: JSON.stringify({
      model: 'claude-3-sonnet-20240229',
      max_tokens: 1024,
      messages: [{ role: 'user', content: prompt }],
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Anthropic API è°ƒç”¨å¤±è´¥: ${response.status} - ${error}`);
  }

  const data = await response.json();
  const content = data.content?.[0]?.text;
  
  if (!content) {
    throw new Error('Anthropic è¿”å›ç©ºå“åº”');
  }

  return parseJsonResponse(content);
}

/**
 * è§£æ JSON å“åº”
 */
function parseJsonResponse(text: string): LLMGradingResult {
  // å°è¯•æå– JSON éƒ¨åˆ†
  const jsonMatch = text.match(/\{[\s\S]*\}/);
  if (!jsonMatch) {
    throw new Error('å“åº”ä¸­æœªæ‰¾åˆ° JSON æ ¼å¼å†…å®¹');
  }

  const parsed = JSON.parse(jsonMatch[0]);
  
  // éªŒè¯å¿…è¦å­—æ®µ
  if (typeof parsed.overall !== 'number' || !parsed.dimensions || !parsed.reasoning) {
    throw new Error('JSON æ ¼å¼ä¸å®Œæ•´ï¼Œç¼ºå°‘å¿…è¦å­—æ®µ');
  }

  return {
    dimensions: parsed.dimensions,
    overall: Math.max(0, Math.min(1, parsed.overall)),
    reasoning: parsed.reasoning,
  };
}

/**
 * æ¨¡æ‹Ÿ LLM è¯„åˆ†ï¼ˆåŸºäºä»£ç ç‰¹å¾çš„å¯å‘å¼è¯„åˆ†ï¼‰
 * ç”¨äºæµ‹è¯•è¯„ä¼°æ¡†æ¶æˆ–æ—  API å¯†é’¥æ—¶çš„åå¤‡æ–¹æ¡ˆ
 */
async function callMockLLM(prompt: string, dimensions: string[]): Promise<LLMGradingResult> {
  // åŸºäºä»£ç å†…å®¹è¿›è¡Œå¯å‘å¼è¯„åˆ†
  const codeLength = prompt.length;
  const hasTypes = prompt.includes('interface') || prompt.includes('type ');
  const hasComments = prompt.includes('//') || prompt.includes('/*');
  const hasErrorHandling = prompt.includes('try') || prompt.includes('catch');
  const hasAgentAware = prompt.includes('agent-aware') || prompt.includes('initAgentAware');
  const hasTailwind = prompt.includes('className') || prompt.includes('tailwind');
  const hasReact = prompt.includes('React') || prompt.includes('useState') || prompt.includes('useEffect');

  // è®¡ç®—åŸºç¡€åˆ†æ•°
  let baseScore = 0.5;
  
  // ä»£ç é‡è¯„åˆ†
  if (codeLength > 500) baseScore += 0.05;
  if (codeLength > 1000) baseScore += 0.05;
  if (codeLength > 2000) baseScore += 0.05;
  
  // ä»£ç è´¨é‡è¯„åˆ†
  if (hasTypes) baseScore += 0.1;
  if (hasComments) baseScore += 0.05;
  if (hasErrorHandling) baseScore += 0.1;
  
  // Agent-Aware é›†æˆè¯„åˆ†
  if (hasAgentAware) baseScore += 0.15;
  
  // UI å®ç°è¯„åˆ†
  if (hasTailwind) baseScore += 0.05;
  if (hasReact) baseScore += 0.05;

  baseScore = Math.min(baseScore, 1);

  // ä¸ºæ¯ä¸ªç»´åº¦ç”Ÿæˆè¯„åˆ†
  const dimensionScores: Record<string, number> = {};
  const reasoning: string[] = [];

  for (const dim of dimensions) {
    let score = baseScore;
    
    // æ ¹æ®ç»´åº¦è°ƒæ•´åˆ†æ•°
    if (dim.includes('ä¾èµ–') || dim.includes('é›†æˆ')) {
      score = hasAgentAware ? Math.min(score + 0.1, 1) : Math.max(score - 0.2, 0);
      reasoning.push(hasAgentAware ? 'agent-aware å·²é›†æˆ' : 'agent-aware æœªé›†æˆ');
    } else if (dim.includes('ä»£ç è´¨é‡') || dim.includes('ä»£ç ç»“æ„')) {
      score = hasTypes ? Math.min(score + 0.05, 1) : score;
      score = hasComments ? Math.min(score + 0.05, 1) : Math.max(score - 0.1, 0);
    } else if (dim.includes('é”™è¯¯å¤„ç†') || dim.includes('é”™è¯¯')) {
      score = hasErrorHandling ? Math.min(score + 0.1, 1) : Math.max(score - 0.15, 0);
    } else if (dim.includes('UI') || dim.includes('æ ·å¼')) {
      score = hasTailwind ? Math.min(score + 0.1, 1) : Math.max(score - 0.1, 0);
    } else if (dim.includes('åˆå§‹åŒ–')) {
      score = hasAgentAware ? Math.min(score + 0.15, 1) : Math.max(score - 0.3, 0);
    }
    
    dimensionScores[dim] = Math.round(score * 100) / 100;
  }

  // è®¡ç®—æ€»åˆ†
  const scores = Object.values(dimensionScores);
  const overall = scores.reduce((a, b) => a + b, 0) / scores.length;

  return {
    dimensions: dimensionScores,
    overall: Math.round(overall * 100) / 100,
    reasoning: `[æ¨¡æ‹Ÿè¯„åˆ†] ä»£ç é•¿åº¦ ${codeLength} å­—ç¬¦ï¼Œ${reasoning.join('ï¼›')}ï¼Œ${hasTypes ? 'æœ‰' : 'æ— '}ç±»å‹å®šä¹‰ï¼Œ${hasComments ? 'æœ‰' : 'æ— '}æ³¨é‡Šï¼Œ${hasErrorHandling ? 'æœ‰' : 'æ— '}é”™è¯¯å¤„ç†`,
  };
}

/**
 * è°ƒç”¨ LLM è¿›è¡Œè¯„åˆ†
 */
async function callLLM(prompt: string, dimensions: string[]): Promise<LLMGradingResult> {
  switch (LLM_BACKEND) {
    case 'openai':
      console.log('ğŸ¤– [LLM Grader] ä½¿ç”¨ OpenAI GPT-4 è¿›è¡Œè¯„ä¼°');
      return await callOpenAI(prompt);
    
    case 'anthropic':
      console.log('ğŸ¤– [LLM Grader] ä½¿ç”¨ Anthropic Claude è¿›è¡Œè¯„ä¼°');
      return await callAnthropic(prompt);
    
    case 'mock':
    default:
      console.log('ğŸ¤– [LLM Grader] ä½¿ç”¨æ¨¡æ‹Ÿè¯„åˆ†ï¼ˆè®¾ç½® EVAL_LLM_BACKEND=openai|anthropic å¯ç”¨çœŸå®è¯„ä¼°ï¼‰');
      return await callMockLLM(prompt, dimensions);
  }
}

/**
 * æ‰§è¡Œ LLM è¯„åˆ†
 * 
 * @param projectDir é¡¹ç›®ç›®å½•
 * @param config LLM è¯„åˆ†å™¨é…ç½®
 * @param codeContent å¯é€‰çš„ä»£ç å†…å®¹ï¼ˆå¦‚æœå·²æ”¶é›†ï¼‰
 */
export async function gradeLLM(
  projectDir: string,
  config: LLMGraderConfig,
  codeContent?: string
): Promise<GraderResult> {
  const details: Record<string, unknown> = {
    rubricLoaded: false,
    codeCollected: false,
    llmCalled: false,
    llmBackend: LLM_BACKEND,
    dimensions: {},
    overall: 0,
    reasoning: '',
  };

  const { rubric, dimensions, threshold = 0.7 } = config;

  try {
    // 1. è¯»å– Rubric
    const rubricContent = readRubric(rubric);
    details.rubricLoaded = !!rubricContent;

    if (!rubricContent) {
      // Rubric æ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†æ ‡å‡†
      console.warn(`âš ï¸ [LLM Grader] Rubric æ–‡ä»¶ä¸å­˜åœ¨: ${rubric}ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†æ ‡å‡†`);
      const defaultRubric = `
## é€šç”¨ä»£ç è´¨é‡è¯„åˆ†æ ‡å‡†

### è¯„ä¼°ç»´åº¦
${dimensions.map(d => `- ${d}`).join('\n')}

### è¯„åˆ†è§„åˆ™
- 1.0 åˆ†ï¼šå®Œå…¨ç¬¦åˆè¦æ±‚ï¼Œä»£ç è´¨é‡ä¼˜ç§€
- 0.7 åˆ†ï¼šåŸºæœ¬ç¬¦åˆè¦æ±‚ï¼Œæœ‰å°é—®é¢˜
- 0.5 åˆ†ï¼šéƒ¨åˆ†ç¬¦åˆè¦æ±‚ï¼Œæœ‰æ˜æ˜¾é—®é¢˜
- 0.3 åˆ†ï¼šä¸ç¬¦åˆè¦æ±‚ï¼Œå­˜åœ¨ä¸¥é‡é—®é¢˜
- 0.0 åˆ†ï¼šå®Œå…¨ä¸ç¬¦åˆè¦æ±‚
      `;
      details.rubricLoaded = true;
      details.usingDefaultRubric = true;
    }

    // 2. æ”¶é›†ä»£ç å†…å®¹ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    let code = codeContent;
    if (!code) {
      code = await collectCodeContent(projectDir);
    }
    details.codeCollected = true;
    details.codeLength = code.length;

    if (code.length < 50) {
      return {
        type: 'llm',
        passed: false,
        score: 0,
        details,
        error: 'ä»£ç å†…å®¹è¿‡å°‘ï¼Œæ— æ³•è¯„ä¼°',
      };
    }

    // 3. æ„å»º Prompt å¹¶è°ƒç”¨ LLM
    const effectiveRubric = rubricContent || `é€šç”¨ä»£ç è´¨é‡è¯„åˆ†æ ‡å‡†ï¼Œè¯„ä¼°ç»´åº¦: ${dimensions.join(', ')}`;
    const prompt = buildEvalPrompt(effectiveRubric, dimensions, code);
    
    console.log(`ğŸ“ [LLM Grader] è¯„ä¼°ç»´åº¦: ${dimensions.join(', ')}`);
    console.log(`ğŸ“ [LLM Grader] ä»£ç é•¿åº¦: ${code.length} å­—ç¬¦`);
    
    const llmResult = await callLLM(prompt, dimensions);
    details.llmCalled = true;

    // 4. è§£æç»“æœ
    details.dimensions = llmResult.dimensions;
    details.overall = llmResult.overall;
    details.reasoning = llmResult.reasoning;

    // 5. è®¡ç®—æ˜¯å¦é€šè¿‡
    const passed = llmResult.overall >= threshold;

    console.log(`ğŸ¯ [LLM Grader] è¯„åˆ†: ${(llmResult.overall * 100).toFixed(0)}%ï¼Œé˜ˆå€¼: ${(threshold * 100).toFixed(0)}%ï¼Œ${passed ? 'âœ… é€šè¿‡' : 'âŒ æœªé€šè¿‡'}`);

    return {
      type: 'llm',
      passed,
      score: llmResult.overall,
      details,
      error: passed ? undefined : `è¯„åˆ† ${(llmResult.overall * 100).toFixed(0)}% ä½äºé˜ˆå€¼ ${(threshold * 100).toFixed(0)}%`,
    };
  } catch (error) {
    console.error(`âŒ [LLM Grader] è¯„åˆ†å¤±è´¥:`, error);
    return {
      type: 'llm',
      passed: false,
      score: 0,
      details,
      error: error instanceof Error ? error.message : String(error),
    };
  }
}
